Voici le **README.md** complet et personnalisÃ© pour le dossier `mcp-server-amdie/`, conforme Ã  ton architecture actuelle et ton fonctionnement rÃ©el :

---

## ğŸ“ `mcp-server-amdie/` â€“ Serveur MCP pour le lancement du backend

Ce module contient le **serveur MCP** utilisÃ© pour dÃ©clencher dynamiquement le backend du chatbot Ã  chaque nouvelle requÃªte utilisateur. Il constitue le **point central dâ€™orchestration** entre lâ€™interface, lâ€™API REST et les outils dâ€™intelligence artificielle embarquÃ©s dans le backend.

---

## ğŸ¯ Objectif

* Recevoir une requÃªte de lâ€™API (question, session, rÃ´le, permissions).
* **Lancer dynamiquement le backend** du chatbot via la fonction `start_backend`.
* Transmettre la rÃ©ponse finale Ã  lâ€™API FastAPI via requÃªte HTTP.
* Servir de **connecteur central** entre le LLM, la base vectorielle, et les rÃ´les dâ€™utilisateurs.

---

## âš™ï¸ Fichiers principaux

| Fichier                      | RÃ´le                                                                                      |
| ---------------------------- | ----------------------------------------------------------------------------------------- |
| `mcp_backend_server.py`      | Contient la configuration du serveur MCP et la fonction `start_backend`.                  |
| `serveur.json`               | Fichier de configuration (manuel ou par test), utilisÃ© Ã©ventuellement par MCP Instructor. |
| `pyproject.toml` / `uv.lock` | Fichiers de gestion des dÃ©pendances.                                                      |
| `__init__.py`                | Initialisation du module.                                                                 |

---

## ğŸš€ DÃ©tails techniques

### DÃ©marrage du serveur MCP

Le serveur utilise le protocole `Streamable-HTTP` et sâ€™expose Ã  :

```
http://0.0.0.0:8090/mcp/
```

Le lancement est fait dans `mcp_backend_server.py` avec :

```python
mcp.run(transport="http", host="0.0.0.0", port=8090, path="/mcp/")
```

---

### Fonction de lancement du backend

```python
async def start_backend(question: str, session_id: str, permissions_csv: str, role: str, username: str, email: str)
```

* Appelle la fonction `_spawn_wrapper(...)` du backend.
* Cette fonction dÃ©marre le traitement LangGraph Ã  la volÃ©e pour la session.
* Retourne la rÃ©ponse formatÃ©e, accompagnÃ©e des messages intermÃ©diaires.

---

## ğŸ”„ Communication

* Le **frontend** appelle lâ€™API (FastAPI).
* Lâ€™**API** appelle le serveur **MCP** via HTTP.
* Le serveur **MCP** appelle le **backend** (`chatbot_wrapper.py`).
* La rÃ©ponse est renvoyÃ©e Ã  lâ€™**API**, puis affichÃ©e Ã  lâ€™utilisateur.

Tous les Ã©changes sont **redirigÃ©s via le MCP** : aucun composant n'appelle directement le backend.

---

## ğŸ§ª Mode test

Il est possible d'utiliser MCP Instructor pour simuler des appels au serveur, en se basant sur la configuration `serveur.json`.

---

Souhaites-tu que je passe maintenant au **README du backend (`backend_python/`)** ?
